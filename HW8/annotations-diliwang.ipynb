{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Annotations, Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going into assignment repo folder:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 cd w205\n",
    "2 ls\n",
    "3 cd assignment-08-dwang-ischool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copying yaml file from course content repo for unit 8:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4 cp ~/w205/course-content//08-Querying-Data/docker-compose.yml ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking that yaml file content is correct:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5 cat docker-compose.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/cdh-minimal:latest\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"50070\" # nn http\n",
    "      - \"8888\" # hue\n",
    "    #ports:\n",
    "    #- \"8888:8888\"\n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.5\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spin up cluster"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6 docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that containers are up:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7 docker-compose ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                Name                            Command            State                                         Ports                                        \n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "assignment08dwangischool_cloudera_1    cdh_startup_script.sh       Up      11000/tcp, 11443/tcp, 19888/tcp, 50070/tcp, 8020/tcp, 8088/tcp, 8888/tcp, 9090/tcp \n",
    "assignment08dwangischool_kafka_1       /etc/confluent/docker/run   Up      29092/tcp, 9092/tcp                                                                \n",
    "assignment08dwangischool_mids_1        /bin/bash                   Up      8888/tcp                                                                           \n",
    "assignment08dwangischool_spark_1       docker-entrypoint.sh bash   Up                                                                                         \n",
    "assignment08dwangischool_zookeeper_1   /etc/confluent/docker/run   Up      2181/tcp, 2888/tcp, 32181/tcp, 3888/tcp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### watch kafka come up"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8 docker-compose logs -f kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check out hdfs before writing to it: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9 docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2018-11-05 14:10 /tmp/hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a kafka topic called assessments:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10 docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --create \\\n",
    "    --topic assessments \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "    --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data set for assessment attempts that we have been using for project 2:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11 curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### publish messages directly into the assessments topic:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "12 docker-compose exec mids bash -c \"cat /w205/assignment-08-dwang-ischool/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spin up the pyspark using spark container:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "13 docker-compose exec spark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inside pyspark, read messages from kafka into raw_assessments variable:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "raw_assessments = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"assessments\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cache to cut back on warnings:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view the schema of the raw_assessments data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cast the values from raw_assessments as strings into the variable assessments"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempt to extract the json fields into variable extracted_assessments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
    "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see what extracted_assessments looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracted_assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output for 20 rows:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
    "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
    "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
    "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
    "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
    "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
    "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
    "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
    "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
    "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
    "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
    "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
    "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
    "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
    "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check out the current schema of extracted_assessments with nesting:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root\n",
    " |-- base_exam_id: string (nullable = true)\n",
    " |-- certification: string (nullable = true)\n",
    " |-- exam_name: string (nullable = true)\n",
    " |-- keen_created_at: string (nullable = true)\n",
    " |-- keen_id: string (nullable = true)\n",
    " |-- keen_timestamp: string (nullable = true)\n",
    " |-- max_attempts: string (nullable = true)\n",
    " |-- sequences: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: array (valueContainsNull = true)\n",
    " |    |    |-- element: map (containsNull = true)\n",
    " |    |    |    |-- key: string\n",
    " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
    " |-- started_at: string (nullable = true)\n",
    " |-- user_exam_id: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to deal with the nested json data, we can use SparkSQL TempTable:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use spark.sql() to create Dataframes from queries of subsets of data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.sql(\"select base_exam_id, started_at, certification,  sequences.questions from assessments limit 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+--------------------+--------------------+-------------+--------------------+\n",
    "|        base_exam_id|          started_at|certification|           questions|\n",
    "+--------------------+--------------------+-------------+--------------------+\n",
    "|37f0a30a-7464-11e...|2018-01-23T14:23:...|        false|[Map(user_incompl...|\n",
    "|37f0a30a-7464-11e...|2018-01-23T14:21:...|        false|[Map(user_incompl...|\n",
    "|4beeac16-bb83-4d5...|2018-01-23T20:22:...|        false|[Map(user_incompl...|\n",
    "|4beeac16-bb83-4d5...|2018-01-23T20:21:...|        false|[Map(user_incompl...|\n",
    "|6442707e-7488-11e...|2018-01-23T19:48:...|        false|[Map(user_incompl...|\n",
    "|8b4488de-43a5-4ff...|2018-01-23T20:51:...|        false|[Map(user_incompl...|\n",
    "|e1f07fac-5566-4fd...|2018-01-23T22:24:...|        false|[Map(user_incompl...|\n",
    "|7e2e0b53-a7ba-458...|2018-01-23T21:43:...|        false|[Map(user_incompl...|\n",
    "|1a233da8-e6e5-48a...|2018-01-23T21:31:...|        false|[Map(user_incompl...|\n",
    "|7e2e0b53-a7ba-458...|2018-01-23T21:42:...|        false|[Map(user_incompl...|\n",
    "|4cdf9b5f-fdb7-4a4...|2018-01-23T21:45:...|        false|[Map(user_incompl...|\n",
    "|e1f07fac-5566-4fd...|2018-01-23T22:24:...|        false|[Map(user_incompl...|\n",
    "|87b4b3f9-3a86-435...|2018-01-23T21:40:...|        false|[Map(user_incompl...|\n",
    "|a7a65ec6-77dc-480...|2018-01-23T21:34:...|        false|[Map(user_incompl...|\n",
    "|7e2e0b53-a7ba-458...|2018-01-23T21:41:...|        false|[Map(user_incompl...|\n",
    "|e5602ceb-6f0d-11e...|2018-01-23T21:51:...|        false|[Map(user_incompl...|\n",
    "|e5602ceb-6f0d-11e...|2018-01-23T21:53:...|        false|[Map(user_incompl...|\n",
    "|f432e2e3-7e3a-4a7...|2018-01-23T21:50:...|        false|[Map(user_incompl...|\n",
    "|76a682de-6f0c-11e...|2018-01-23T21:46:...|        false|[Map(user_incompl...|\n",
    "|a7a65ec6-77dc-480...|2018-01-23T21:35:...|        false|[Map(user_incompl...|\n",
    "+--------------------+--------------------+-------------+--------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the above data frame into hdfs:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "some_assessments_data = spark.sql(\"select base_exam_id, started_at, certification,  sequences.questions from assessments limit 20\")\n",
    "\n",
    "some_assessments_data.write.parquet(\"/tmp/some_assessments_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in a different terminal window, I check to see the results in hadoop of the file that was just written.  Please see diliwang-history2 for the following line:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "892 docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**in the output, we see 3 items instead of the previous 2.  The third item is the some_assessments_data parquet that we just wrote**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 3 items\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2018-11-05 14:10 /tmp/hive\n",
    "drwxr-xr-x   - root   supergroup          0 2018-11-05 14:54 /tmp/some_assessments_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at the third file further (diliwang-history2.txt):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "893 docker-compose exec cloudera hadoop fs -ls /tmp/some_assessments_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   1 root supergroup          0 2018-11-05 14:54 /tmp/some_assessments_data/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup       2646 2018-11-05 14:54 /tmp/some_assessments_data/part-00000-5a9eed59-1c97-468b-b132-2d9b69b644cd-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In addition, I could have done the data extraction in a different way, using the Row() function, following the world cup players example from class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following allows us to deal with unicode:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf8', buffering=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract the data again into different variable extracted_assessments2.  This time, I will be using Row:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "extracted_assessments2 = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taking a look at extracted_assessments2:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extracted_assessments2.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output for 20 rows:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
    "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
    "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
    "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
    "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
    "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
    "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
    "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
    "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
    "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
    "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
    "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
    "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
    "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
    "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is already a dataframe, so we can write directly to hdfs:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "extracted_assessments2.write.parquet(\"/tmp/extracted_assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at the result in hadoop in a different terminal window (diliwang-history2.txt):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "894 docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ouptut:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 4 items\n",
    "drwxr-xr-x   - root   supergroup          0 2018-11-05 15:11 /tmp/extracted_assessments\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2018-11-05 14:10 /tmp/hive\n",
    "drwxr-xr-x   - root   supergroup          0 2018-11-05 14:54 /tmp/some_assessments_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "895 docker-compose exec cloudera hadoop fs -ls /tmp/extracted_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ouptut:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Found 2 items\n",
    "-rw-r--r--   1 root supergroup          0 2018-11-05 15:11 /tmp/extracted_assessments/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup     345388 2018-11-05 15:11 /tmp/extracted_assessments/part-00000-84cbdf79-25a5-40a3-9abe-32051ba0c50c-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**as expected, we see a second parquet file in hadoop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back in pyspark, I exit:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking out my pyspark history:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "15 docker-compose exec spark cat /root/.python_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**output:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "raw_assessments = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"assessments\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() \n",
    "raw_assessments.cache()\n",
    "raw_assessments.printSchema()\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "import json\n",
    "extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF()\n",
    "extracted_assessments.show()\n",
    "extracted_assessments.printSchema()\n",
    "extracted_assessments.registerTempTable('assessments')\n",
    "extracted_assessments.show()\n",
    "spark.sql(\"select base_exam_id from assessments limit 10\")\n",
    "spark.sql(\"select base_exam_id from assessments limit 10\").show()\n",
    "spark.sql(\"select base_exam_id, sequence.value.value from assessments limit 10\").show()\n",
    "spark.sql(\"select base_exam_id, sequences.value.value from assessments limit 10\").show()\n",
    "spark.sql(\"select base_exam_id, sequences, starting_at from assessments limit 10\").show()\n",
    "spark.sql(\"select base_exam_id, started_at, sequences from assessments limit 10\").show()\n",
    "spark.sql(\"select base_exam_id, started_at, sequences.questions from assessments limit 10\").show()\n",
    "spark.sql(\"select base_exam_id, started_at, certification,  sequences.questions from assessments limit 10\").show()\n",
    "spark.sql(\"select base_exam_id, started_at, certification,  sequences.questions from assessments limit 20\").show()\n",
    "some_assessments_data = spark.sql(\"select base_exam_id, started_at, certification,  sequences.questions from assessments limit 20\").show()\n",
    "some_assessments_data.write.parquet(\"/tmp/some_assessments_data\")\n",
    "some_assessments_data.show()\n",
    "some_assessments_data = spark.sql(\"select base_exam_id, started_at, certification,  sequences.questions from assessments limit 20\").show()\n",
    "some_assessments_data.show()\n",
    "some_assessments_data = spark.sql(\"select base_exam_id, started_at, certification,  sequences.questions from assessments limit 20\")\n",
    "some_assessments_data.show()\n",
    "some_assessments_data.write.parquet(\"/tmp/some_assessments_data\")\n",
    "from pyspark.sql import Row\n",
    "extracted_assessments2 = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "extracted_assessments2.show()\n",
    "extracted_assessments2.write.parquet(\"/tmp/extracted_assessments\")\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### storing my pyspark history in my homework 8 repo as pyspark_history.txt file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "16 docker-compose exec spark cat /root/.python_history > pyspark_history.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taking down my containers:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "17 docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing the history of my primary terminal window commands to a text file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "18 history > diliwang-history.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing the history of my secondary terminal window commands to a text file.  The secondary window is where I checked hadoop for files that I had written (diliwang-history2.txt):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "896 history > diliwang-history2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
